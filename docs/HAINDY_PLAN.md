# Autonomous AI Testing Agent â€” High-Level Implementation Plan

## 1  MVP Goals

| # | Goal | Success criterion | Status |
|---|------|-------------------|---------|
| 1 | Accept high-level requirements (PRD, test scenarios) and autonomously execute testing workflows. | Agent successfully completes 3/5 different test scenarios without human intervention. | ðŸ”„ In Progress |
| 2 | Multi-agent coordination: Test planning â†’ execution â†’ evaluation â†’ iteration. | Each agent fulfills its role with >80% accuracy in its domain. | âœ… Achieved |
| 3 | Grid-based browser interaction for reliable cross-application compatibility. | Successfully navigates and interacts with 5+ different web applications. | âœ… Achieved |
| 4 | Comprehensive test execution reporting with evidence collection. | Generates detailed reports with screenshots, steps, and outcomes for each test run. | âœ… Achieved |
| 5 | Debug visibility and AI interaction logging. | Complete visibility into AI prompts, responses, and decision-making process. | âœ… Achieved |

---

## 2  Tech Stack

| Layer | Technology | Why | Status |
|-------|------------|-----|--------|
| Orchestrator | **Python 3.10+** | Best ecosystem for AI & agent coordination. | âœ… |
| Browser driver | **Playwright-Python (Chromium)** | WebSocket/CDP, native screenshots & absolute mouse clicks. | âœ… |
| AI Agents | **4x OpenAI GPT-4o mini instances** | Multi-agent system: Planner, Runner, Action, Evaluator agents. | âš ï¸ Vision limitations |
| Agent coordination | **Custom multi-agent framework** | Manages agent communication, state, and workflow orchestration. | âœ… |
| Grid system | **Adaptive grid overlay + coordinate mapping** | DOM-free visual interaction with adaptive refinement (60Ã—60 grid). | âœ… |
| Test scenarios | **Natural language requirements** | PRDs, user stories, test case descriptions as input. | âœ… |
| Debug logging | **JSONL + HTML reports** | AI conversations, screenshots, comprehensive debugging. | âœ… |
| Packaging | `setuptools` + `pyproject.toml` | Simple `pip` distribution. | âœ… |

---

## 3  Folder Structure

```
autonomous-ai-testing-agent/
â”‚
â”œâ”€ src/
â”‚   â”œâ”€ agents/             # AI agent implementations
â”‚   â”‚   â”œâ”€ __init__.py
â”‚   â”‚   â”œâ”€ base_agent.py   # Abstract base class for all agents
â”‚   â”‚   â”œâ”€ test_planner.py # Requirements â†’ Test plan agent
â”‚   â”‚   â”œâ”€ test_runner.py  # Test orchestration & coordination agent
â”‚   â”‚   â”œâ”€ action_agent.py # Screenshot + instruction â†’ grid coordinates
â”‚   â”‚   â””â”€ evaluator.py    # Screenshot + expectation â†’ success/failure
â”‚   â”œâ”€ orchestration/      # Multi-agent coordination
â”‚   â”‚   â”œâ”€ __init__.py
â”‚   â”‚   â”œâ”€ coordinator.py  # Agent workflow management
â”‚   â”‚   â”œâ”€ communication.py # Inter-agent message passing
â”‚   â”‚   â””â”€ state_manager.py # Test execution state tracking
â”‚   â”œâ”€ browser/            # Browser automation layer
â”‚   â”‚   â”œâ”€ __init__.py
â”‚   â”‚   â”œâ”€ driver.py       # Playwright wrapper (click, scroll, screenshot)
â”‚   â”‚   â””â”€ controller.py   # High-level browser operations
â”‚   â”œâ”€ grid/
â”‚   â”‚   â”œâ”€ __init__.py
â”‚   â”‚   â”œâ”€ overlay.py      # Adaptive grid overlay utilities with refinement
â”‚   â”‚   â”œâ”€ coordinator.py  # Grid coordinate mapping and adaptive refinement
â”‚   â”‚   â”œâ”€ refinement.py   # Zoom-in and sub-grid analysis logic
â”‚   â”‚   â””â”€ utils.py
â”‚   â”œâ”€ models/
â”‚   â”‚   â”œâ”€ __init__.py
â”‚   â”‚   â”œâ”€ openai_client.py # OpenAI API wrapper
â”‚   â”‚   â””â”€ prompts.py      # System prompts for each agent type
â”‚   â”œâ”€ core/               # abstraction layers & interfaces
â”‚   â”‚   â”œâ”€ __init__.py
â”‚   â”‚   â”œâ”€ interfaces.py   # Agent, BrowserDriver, TestExecutor abstracts
â”‚   â”‚   â”œâ”€ middleware.py   # logging, monitoring, error handling middleware
â”‚   â”‚   â””â”€ types.py        # Data models (TestPlan, TestStep, ActionResult)
â”‚   â”œâ”€ error_handling/
â”‚   â”‚   â”œâ”€ __init__.py
â”‚   â”‚   â”œâ”€ recovery.py     # retry logic, hallucination detection
â”‚   â”‚   â””â”€ validation.py   # confidence scoring, action validation
â”‚   â”œâ”€ security/
â”‚   â”‚   â”œâ”€ __init__.py
â”‚   â”‚   â”œâ”€ rate_limiter.py # DoS prevention
â”‚   â”‚   â””â”€ sanitizer.py    # sensitive data protection
â”‚   â”œâ”€ monitoring/
â”‚   â”‚   â”œâ”€ __init__.py
â”‚   â”‚   â”œâ”€ logger.py       # structured logging (stdout + file)
â”‚   â”‚   â”œâ”€ analytics.py    # success/failure tracking
â”‚   â”‚   â””â”€ reporter.py     # test execution reports generation
â”‚   â””â”€ config/
â”‚       â”œâ”€ __init__.py
â”‚       â”œâ”€ settings.py     # environment-specific configs
â”‚       â”œâ”€ agent_prompts.py # System prompts configuration
â”‚       â””â”€ validation.py   # config validation
â”‚
â”œâ”€ tests/                  # unit tests & integration tests
â”œâ”€ test_scenarios/         # example test scenarios & PRDs
â”œâ”€ data/                   # captured screenshots & logs
â”œâ”€ reports/                # test execution reports & analytics
â”œâ”€ docs/                   # this file and extra docs
â””â”€ pyproject.toml
```

---

## 4  Multi-Agent Architecture

### **4.1 AI Agent System**
```python
# Four specialized AI agents working in coordination
class TestPlannerAgent(BaseAgent):
    """Analyzes requirements/PRDs â†’ Creates structured test plans"""
    def create_test_plan(self, requirements: str) -> TestPlan: pass

class TestRunnerAgent(BaseAgent):
    """Orchestrates test execution â†’ Decides next steps"""  
    def get_next_action(self, test_plan: TestPlan, current_state: TestState) -> ActionInstruction: pass
    def evaluate_step_result(self, step_result: StepResult) -> TestState: pass

class ActionAgent(BaseAgent):
    """Screenshot + instruction â†’ Adaptive grid coordinates with refinement"""
    def determine_action(self, screenshot: bytes, instruction: str) -> GridAction: pass
    def refine_coordinates(self, cropped_region: bytes, initial_coords: GridCoords) -> RefinedGridAction: pass

class EvaluatorAgent(BaseAgent):
    """Screenshot + expectation â†’ Success/failure assessment"""
    def evaluate_result(self, screenshot: bytes, expected_outcome: str) -> EvaluationResult: pass
```

### **4.2 Agent Coordination Workflow**
```python
# Multi-agent execution flow
Human Input: "Test checkout flow for Product X"
    â†“
TestPlannerAgent: Creates structured test plan (8 steps)
    â†“
TestRunnerAgent: "Step 1: Navigate to product page"
    â†“
ActionAgent: Analyzes screenshot â†’ "Click grid cell B7"
    â†“
BrowserDriver: Executes action, waits, captures screenshot  
    â†“
EvaluatorAgent: "Success - product page loaded correctly"
    â†“
TestRunnerAgent: Processes result â†’ "Step 2: Add to cart"
    â†“
[Loop continues until test completion]
```

### **4.3 System Prompts & Agent Specialization**
- **Test Planner**: Understands business requirements, creates comprehensive test plans
- **Test Runner**: Maintains test context, coordinates execution, handles branching logic, manages scripted automation fallbacks
- **Action Agent**: Adaptive grid specialist, visual refinement expert, confidence-based precision targeting
- **Evaluator**: Result validation, UI state assessment, error detection, confidence scoring

### **4.4 Error Handling & Recovery**
- **Agent-level error handling**: Each agent validates its own outputs
- **Cross-agent validation**: Results validated by subsequent agents
- **Retry logic**: Max 3 attempts per action with agent coordination
- **Fallback strategies**: Alternative action paths when primary approaches fail

### **4.5 State Management**
- **Test execution state**: Current step, completed steps, remaining steps
- **Browser state tracking**: Page loaded, navigation state, UI changes
- **Agent communication state**: Message passing, result sharing
- **Test plan state**: Dynamic plan updates based on discovered UI changes

### **4.6 Security & Safety**
- **Rate limiting**: Coordinated API throttling across all agents
- **Sensitive data protection**: PII detection before screenshot sharing
- **Sandboxed execution**: Browser isolation, controlled navigation scope

### **4.7 Observability & Debugging**
- **Multi-agent logging**: Track communication between agents
- **Test execution reports**: Complete workflow documentation
- **Agent performance metrics**: Individual agent success rates
- **Visual evidence**: Screenshot captures at each decision point
- **Execution timeline**: Full audit trail with timestamps and decisions

---

## 5  Grid-Based Interaction and Adaptive Refinement

### **5.1 DOM-Free Interaction Approach**

**Core Philosophy**: The system explicitly avoids DOM-based interaction methods (XPath, CSS selectors, element IDs) at all stages of development, including the MVP and future roadmap.

**Rationale**:
- **Brittleness Elimination**: DOM-based selectors introduce maintenance overhead and break easily with UI changes
- **No-Code Accessibility**: Enables users without technical expertise to use the system effectively
- **Platform Extensibility**: Visual-only approach enables future expansion to mobile apps, desktop applications, and platforms without reliable DOM access
- **Reliability**: Visual interaction patterns are more stable across different frameworks and implementations

### **5.2 Adaptive Grid System**

**Initial Grid Configuration**:
- Base grid: 60Ã—60 overlay (tentative, subject to optimization through testing)
- Grid cells numbered/lettered for easy reference (A1-Z60, AA1-AZ60, etc.)
- Visual overlay with semi-transparent grid lines for debugging/development

**Adaptive Refinement Strategy**:
When initial grid selection yields uncertain or borderline results:

1. **Automatic Zoom-In**: Agent crops the selected cell and its 8 neighboring cells
2. **Sub-Grid Analysis**: AI re-analyzes the cropped 3Ã—3 region with higher granularity
3. **Precision Targeting**: Agent pinpoints exact coordinates or corners within the refined area
4. **Confidence Validation**: Higher precision improves confidence scores significantly beyond initial grid limitations

### **5.3 Example Adaptive Refinement Workflow**

```
Step 1: Initial Analysis
- AI receives full screenshot with 60Ã—60 grid overlay
- Action Agent analyzes: "Click the 'Add to Cart' button"
- Initial selection: Grid cell M23 (confidence: 70%)

Step 2: Refinement Trigger
- Confidence below threshold (80%) triggers refinement
- System crops 3Ã—3 region: L22, L23, L24, M22, M23, M24, N22, N23, N24
- Cropped region analyzed at higher resolution

Step 3: Precision Targeting
- AI re-analyzes cropped region: "Button clearly visible in center-right of M23"
- Refined coordinates: M23 + offset (0.7, 0.4) relative to cell
- New confidence: 95%

Step 4: Action Execution
- Click executed at refined coordinates
- Action logged with both initial and refined coordinate details
```

### **5.4 Grid System Implementation Details**

**Coordinate Mapping**:
- Grid cells mapped to absolute pixel coordinates
- Support for fractional coordinates within cells (e.g., A1.5.7 = 50% right, 70% down in cell A1)
- Adaptive scaling based on viewport dimensions

**Visual Feedback**:
- Development mode: Grid overlay visible with cell labels
- Production mode: Grid overlay hidden, coordinates calculated transparently
- Screenshot capture includes grid reference for debugging

---

## 6  Detailed Execution Journaling

### **6.1 Structured Natural Language Logging**

Each test execution step must generate comprehensive, human-readable logs with the following structure:

```json
{
  "timestamp": "2024-01-15T10:30:45.123Z",
  "test_scenario": "E-commerce Checkout Flow",
  "step_reference": "Step 3: Add product to cart",
  "action_taken": "Clicked 'Add to Cart' button using adaptive grid refinement",
  "grid_coordinates": {
    "initial_selection": "M23",
    "initial_confidence": 0.70,
    "refinement_applied": true,
    "refined_coordinates": "M23+offset(0.7,0.4)",
    "final_confidence": 0.95
  },
  "expected_result": "Product added to cart, cart counter increments",
  "actual_result": "Cart counter changed from 0 to 1, success notification appeared",
  "agent_confidence": 0.95,
  "screenshot_before": "screenshots/step_3_before.png",
  "screenshot_after": "screenshots/step_3_after.png",
  "execution_time_ms": 1247,
  "success": true
}
```

### **6.2 Caching and Reuse Strategy**

**Action Pattern Recognition**:
- Successful actions stored as reusable patterns
- Pattern matching for similar UI elements across different test runs
- Adaptive pattern library builds over time

**Execution Journal Benefits**:
- Human-readable test execution history
- Debugging aid for failed test scenarios
- Pattern recognition for UI consistency
- Performance optimization through action caching

---

## 7  Just-In-Time Scripted Automation

### **7.1 Dual-Mode Execution Strategy**

**Primary Mode: Scripted Replay**
- Each successful AI action recorded as explicit WebDriver (Playwright) API calls
- Subsequent test executions attempt direct WebDriver command replay first
- Significantly faster execution for stable UI elements

**Fallback Mode: Visual AI Interaction**
- When scripted commands fail (UI changes, element not found, etc.)
- System seamlessly reverts to visual-grid AI interaction
- Updated action recorded for future use

### **7.2 Action Recording Format**

```python
# Example recorded action
{
  "action_type": "click",
  "playwright_command": "page.click('button[data-testid=\"add-to-cart\"]')",
  "visual_backup": {
    "grid_coordinates": "M23+offset(0.7,0.4)",
    "screenshot_reference": "add_to_cart_button_reference.png",
    "confidence_threshold": 0.85
  },
  "success_criteria": "Cart counter increments",
  "timestamp": "2024-01-15T10:30:45.123Z"
}
```

### **7.3 Hybrid Execution Flow**

```
1. Load Test Scenario
2. Check for existing scripted actions
3. Attempt scripted replay (WebDriver commands)
4. IF scripted action fails:
   a. Capture current screenshot
   b. Invoke visual AI interaction
   c. Record new action for future use
5. Continue with next step
```

---

## 8  Hierarchical Agent Validation Strategy

### **8.1 Layered Validation Architecture**

**Bottom-Up Validation**:
- Action-level agents return results with confidence scores
- Each agent validates its own output before passing upstream
- Confidence thresholds trigger retry or escalation

**Top-Down Confirmation**:
- Higher-level agents (Test Runner, Planner) verify lower-level outputs
- Contextual consistency checks across agent decisions
- Cross-agent validation prevents cascading errors

### **8.2 Confidence Threshold System**

| Confidence Level | Action Taken | Escalation |
|------------------|--------------|------------|
| 95-100% | Execute immediately | None |
| 80-94% | Execute with monitoring | Log for review |
| 60-79% | Trigger adaptive refinement | Retry with refinement |
| 40-59% | Request human guidance | Pause for intervention |
| 0-39% | Fail gracefully | Escalate to human |

### **8.3 Hallucination Mitigation**

**Cross-Agent Verification**:
- Action Agent decisions validated by Evaluator Agent
- Test Runner Agent maintains execution context consistency
- Planner Agent validates step sequence logical flow

**Confidence Scoring**:
- Multi-factor confidence calculation (visual clarity, context match, historical success)
- Confidence degradation triggers additional validation layers
- Human-in-the-loop triggers for persistent low confidence

**Validation Checkpoints**:
- Pre-action validation: "Is this action appropriate for current context?"
- Post-action validation: "Did the action achieve expected outcome?"
- Sequence validation: "Does current state align with test plan progression?"

---

## 8.5  Phase 11b: CLI Improvements

### **Redesigned CLI Interface**

The current CLI requires typing test requirements as command-line arguments, which is impractical for real-world usage. Phase 11b redesigns the interface to match how QA engineers actually work.

**New CLI Commands**:

1. **Interactive Requirements Mode**:
   ```bash
   python -m src.main --requirements
   ```
   Opens an interactive prompt where users can paste or type multi-line test requirements

2. **File-based Plan Input**:
   ```bash
   python -m src.main --plan <file>
   ```
   - Passes file directly to the AI model (OpenAI can read most formats)
   - Model extracts requirements from any document type
   - Automatically generates a JSON test scenario file for reuse
   - Outputs: `test_scenarios/generated_<timestamp>.json`

3. **Direct JSON Execution**:
   ```bash
   python -m src.main --json-test-plan <json-file>
   ```
   Points to an existing JSON test scenario file

4. **Utility Commands**:
   ```bash
   python -m src.main --test-api    # Tests OpenAI API key configuration
   python -m src.main --version     # Shows version (0.1.0)
   python -m src.main --help        # Comprehensive help with examples
   ```

5. **Berserk Mode**:
   ```bash
   python -m src.main --berserk --plan requirements.md
   ```
   - Attempts to complete all tests without human intervention
   - Aggressive retry strategies
   - Auto-recovery from errors
   - Skips confirmations and warnings

**Implementation Details**:
- Use `click` or enhance argparse for better CLI UX
- Pass files directly to OpenAI API (let the model handle format parsing)
- Implement interactive prompt with multi-line support
- Auto-generate descriptive JSON filenames
- Add proper version management

---

## 9  Development Phases

| Phase | Tasks | ETA |
|-------|-------|-----|
| **0 â€” Prep** | Repo scaffold, `pre-commit`, multi-agent project structure. | 1 day |
| **1 â€” Core foundation** | Base agent class, OpenAI client, data models (TestPlan, TestStep, etc.). | 2 days |
| **2 â€” Browser & Grid system** | Playwright wrapper, adaptive grid overlay, coordinate mapping with refinement. | 2-3 days |
| **3 â€” Test Planner Agent** | Requirements analysis, test plan generation, system prompts. | 2-3 days |
| **4 â€” Action Agent** | Screenshot analysis, adaptive grid refinement, precision coordinate determination. | 2-3 days |
| **5 â€” Evaluator Agent** | Result assessment, success/failure detection, UI state validation. | 2 days |
| **6 â€” Test Runner Agent** | Test orchestration, step coordination, execution flow management. | 3 days |
| **7 â€” Agent Coordination** | Inter-agent communication, state management, workflow orchestration. | 2-3 days |
| **8 â€” Execution Journaling & Scripted Automation** | Detailed logging, action recording, just-in-time script generation. | 2-3 days |
| **9 â€” Error Handling & Recovery** | Agent-level error handling, retry logic, fallback strategies. | 2 days |
| **10 â€” Security & Monitoring** | Rate limiting, logging, analytics, execution reporting. | 2 days |
| **10b â€” Test Suite Cleanup** | Fix all failing tests, ensure 100% pass rate before Phase 11. | 0.5 days |
| **11 â€” End-to-end Integration** | Complete multi-agent workflow testing and debugging. | 3 days |
| **11b â€” CLI Improvements** | Redesign CLI for real-world usage: interactive prompts, file input support, JSON generation. | 1-2 days |
| **12 â€” Test Scenarios** | Create 5 comprehensive test scenarios, measure success rates. | 2-3 days |
| **13 â€” Packaging & docs** | CLI interface, documentation, release v0.1.0. | 1-2 days |

_Total: roughly **28â€“36 working days**._

---

## 10  Design Principles

1. **Agent specialization** â€“ Each AI agent has a focused role and expertise domain.  
2. **Coordinated intelligence** â€“ Agents collaborate but maintain independent decision-making.  
3. **Fail-fast with recovery** â€“ Agent-level validation with cross-agent verification and rollback.  
4. **Observable by default** â€“ Every agent communication and decision must be traceable.  
5. **Human-interpretable** â€“ Test plans, actions, and results must be understandable to humans.
6. **Modular & extensible** â€“ Easy to add new agent types or replace existing ones.
7. **Production-ready** â€“ Built for reliability, scale, and real-world testing scenarios.

---

## 11  Post-MVP Roadmap

| Milestone | Description |
|-----------|-------------|
| **v0.2** | Multi-tab support, complex form handling, file upload capabilities. |
| **v0.3** | Specialized agents: API testing agent, mobile testing agent, accessibility testing agent. |
| **v0.4** | Learning & adaptation: Agent fine-tuning based on execution history and failure analysis. |
| **v0.5** | REST API for remote test execution, CI/CD integration, collaborative test planning. |
| **v0.6** | Mobile and desktop application testing, visual regression testing, cross-platform grid adaptation. |

---

## 12  Known Risks & Mitigations

| Risk | Mitigation |
|------|-----------|
| Browser update breaks CDP API. | Pin Playwright â‰¥ 1.45; daily CI smoke tests. |
| API rate limits or downtime. | Implement retry logic, exponential backoff, coordinate rate limiting across agents. |
| Agent coordination failures. | Circuit breaker pattern, fallback to single-agent mode, timeout handling. |
| Infinite loops in test execution. | Global `step_limit`, watchdog timers, agent-level loop detection. |
| Agent hallucination cascade. | Cross-agent validation, confidence thresholds, human-in-loop triggers. |
| Miss-clicks in dense UIs. | Dynamic grid scaling, element detection fallbacks, agent coordination for complex interactions. |

---

## 13  Immediate Next Steps

1. Add this `PLAN.md` to `docs/`.  
2. Create `conda`/`venv` with Python 3.10.  
3. `pip install playwright openai && playwright install chromium`.  
4. Build minimal browser wrapper; verify grid overlay and coordinate mapping.  
5. Implement first Test Planner Agent with basic requirements â†’ test plan functionality.
6. Create initial system prompts for each agent type.
7. Test single-agent workflows before building inter-agent coordination.

---

## 14  Progress Tracking

### âœ… Completed Phases (16/18)

For detailed information about completed phases, see [COMPLETED_PHASES_DETAILS.md](./COMPLETED_PHASES_DETAILS.md).

| Phase | Description | Key Achievement |
|-------|-------------|-----------------|
| **0-11b** | Core system implementation | Full multi-agent testing framework operational |
| **Architecture Refactor** | Action Agent owns execution lifecycle | Improved debugging and error handling |
| **Debug Enhancement** | AI interaction logging & screenshot management | Complete visibility into test execution |

### ðŸ”„ In Progress

#### Phase 12 â€” Test Scenarios

| Status | Target | Progress |
|--------|--------|----------|
| ðŸ”„ In Progress | This week | 1/5 scenarios working |

**Current State**:
- Created 5 comprehensive test scenarios
- Wikipedia search scenario demonstrates all action types
- Known limitation: GPT-4o-mini vision capabilities with multiple images

**Remaining Work**:
- Test remaining 4 scenarios
- Document scenario creation best practices

**Note**: Typing validation issues have been resolved with o4-mini model

### ðŸ“… Upcoming Phases

#### Phase 13 â€” Conversation-Based AI Interactions

| Status | Target | ETA |
|--------|--------|-----|
| ðŸ“… Planned | Next | 3-4 days |

**Problem Statement**:
Currently, each AI call is isolated without context from previous interactions. This leads to:
- Redundant information in every prompt
- Inability to reference previous analyses
- Complex workarounds for image comparison
- GPT-4o-mini limitations with multiple images

**Proposed Solution**:
Implement conversation threads for Action Agent where:
- **One conversation per action** - New action = new conversation
- AI remembers previous screenshots and analyses within the same action
- Natural comparison between states (before/after)
- Single image per message (works with GPT-4o-mini)
- Errors and retries remain part of the conversation

**Implementation Plan**:

1. **Conversation State Management** (1 day)
   - Add `conversation_history` to ActionAgent
   - Implement message history tracking per action (not per step)
   - Token-based sliding window with raw message storage (no compression)
   - Reset conversation when moving to next action

2. **Replace AI Interaction Code** (1-2 days)
   - **Replace** (not wrap) existing `call_openai_with_debug` implementation
   - Build conversation-aware OpenAI calls from scratch
   - Include all messages in conversation (including errors/retries)
   - Implement token-based context window management

3. **Optimize Prompts** (1 day)
   - Rewrite prompts to leverage conversation context
   - Remove redundant information
   - Implement natural language transitions between attempts

4. **Testing & Validation** (1 day)
   - Test with Wikipedia scenario
   - Verify improved accuracy
   - Measure token usage optimization

**Scope**:
- **Action Agent only** - Other agents will be addressed in future phases if needed
- No wrappers or fallbacks - direct replacement of existing code
- Conversation boundary: Start of action â†’ End of action (success or final failure)

**Success Criteria**:
- AI accurately references previous screenshots within an action
- Successful visual comparison without sending multiple images
- Improved typing detection accuracy
- Reduced token usage by 30%+
- Clean conversation reset between actions

#### Phase 14 â€” Additional Action Types

| Status | Target | ETA |
|--------|--------|-----|
| ðŸ“… Planned | After Phase 13 | 2-3 days |

**Detailed Plan**: See [PHASE_14_PLAN.md](./PHASE_14_PLAN.md) for comprehensive implementation details and progress tracking.

**Problem Statement**:
Current limitations prevent testing of content below the viewport and complex interactions.

**Action Types to Implement**:

1. **Scroll Actions** (Priority 1)
   - Scroll to element
   - Scroll by pixels
   - Scroll to bottom/top
   - Horizontal scrolling

2. **Extended Interactions** (Priority 2)
   - Hover/mouse over
   - Drag and drop
   - Right-click/context menu
   - Double-click

3. **Form Interactions** (Priority 3)
   - Select dropdown options
   - File upload
   - Checkbox/radio button groups
   - Date picker interactions

4. **Validation Actions** (Priority 2)
   - URL validation (programmatic, not visual)
   - Page title validation
   - Browser state validation

**Implementation Approach**:
- Extend TestStep action_type enum
- Add corresponding workflows in ActionAgent
- Update browser driver with new capabilities
- Test with scenarios requiring these actions

**Success Criteria**:
- Wikipedia test can verify sections below viewport
- Can interact with dropdown menus
- Can handle complex form interactions

#### Phase 15 â€” Packaging & Documentation

| Status | Target | ETA |
|--------|--------|-----|
| ðŸ“… Planned | After Phase 14 | 1-2 days |

**Tasks**:
- PyPI package preparation
- Comprehensive API documentation
- User guide and tutorials
- Release v0.1.0

### Key Metrics

| Metric | Current | Target |
|--------|---------|--------|
| Test Coverage | 81% | 80%+ âœ… |
| Code Quality | Zero warnings | Maintained âœ… |
| Test Success Rate | 20% (1/5 scenarios) | 60%+ (3/5 scenarios) |
| AI Vision Accuracy | Improved with o4-mini | High | 